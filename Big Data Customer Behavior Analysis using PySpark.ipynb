{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUH10nk_3CWZ"
   },
   "source": [
    "# Customer Behaviour Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By Aayush Singh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzsjLp43zEFw"
   },
   "source": [
    "# Objective\n",
    "In this case study, you will be working on E-commerce Customer Behavior Analysis using Apache Spark, a powerful distributed computing framework designed for big data processing. This assignment aims to give you hands-on experience in analyzing large-scale e-commerce datasets using PySpark. You will apply techniques learned in data analytics to clean, transform, and explore customer behavior data, drawing meaningful insights to support business decision-making. Apart from understanding how big data tools can optimize performance on a single machine and across clusters, you will develop a structured approach to analyzing customer segmentation, purchase patterns, and behavioral trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsiRheDOzFh1"
   },
   "source": [
    "# Business Value\n",
    "E-commerce businesses operate in a highly competitive market where understanding customer behavior is critical to driving growth and retention. To stay ahead, companies must leverage data-driven insights to optimize marketing strategies, personalize customer experiences, and improve product offerings. In this assignment, you will analyze e-commerce transaction data to uncover patterns in purchasing behavior, customer preferences, and sales performance. With Apache Spark's ability to handle large datasets efficiently, businesses can process vast amounts of customer interactions in real-time, helping them make faster and more informed decisions.\n",
    "As an analyst at an e-commerce company, your task is to examine historical transaction records and customer survey data to derive actionable insights that can drive business growth. Your analysis will help identify high-value customers, segment users based on behavior, and uncover trends in product demand and customer engagement. By leveraging big data analytics, businesses can enhance customer satisfaction, improve retention rates, and maximize revenue opportunities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTIS-7h9zG0b"
   },
   "source": [
    "# Tasks\n",
    "1. Data Preparation\n",
    "2. Data Cleaning\n",
    "3. Exploratory Data Analysis\n",
    "4. Evaluation and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DG9z6-swb2Q"
   },
   "source": [
    "\n",
    "# Dataset Overview\n",
    "The dataset can be accessed the following [link](https://drive.google.com/drive/folders/1mBgC5tvZrh1bIBvpXVP_j-au5LFUAwOZ?usp=sharing).\n",
    "\n",
    "The dataset used in this analysis comprises longitudinal purchase records from 5,027 Amazon.com users in the United States, spanning 2018 to 2022.\n",
    "\n",
    "It is structured into three CSV files (amazon-purchases.csv, survey.csv, and fields.csv) that capture transactional data, demographic profiles, and survey responses.\n",
    "\n",
    "Collected with informed consent, the dataset enables analysis of customer behavior, product preferences, and demographic trends.\n",
    "\n",
    "**NOTE**: Personal identifiers (PII) were removed to ensure privacy, and all data were preprocessed by users before submission.\n",
    "\n",
    "`Data Dictionary:`\n",
    "\n",
    "| **Attribute**          | **Description** |\n",
    "|------------------------|----------------|\n",
    "| **Order Dates**        | The specific dates when orders were placed, enabling chronological analysis of sales trends. |\n",
    "| **Title** |The name of the product purchased. |\n",
    "|**Category** | The classification or group to which the product belongs, facilitating category-wise analysis. |\n",
    "| **Pricing** | The cost per unit of each product, essential for revenue calculations and pricing strategy assessments. |\n",
    "| **Quantities** | The number of units of each product ordered in a transaction, aiding in inventory and demand analysis. |\n",
    "| **Shipping States**    | The states to which products were shipped, useful for geographical sales distribution analysis. |\n",
    "| **Survey ResponseID**  | A unique identifier linking purchases to customer survey responses, enabling correlation between purchasing behavior and customer feedback. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fokhVB-gOv1N"
   },
   "source": [
    "# Loading the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vYgUNwWIXdbS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "## Installing the libraries if required\n",
    "!pip install --quiet pyspark==3.5.4 datasets==3.3.2 pandas==2.2.2 matplotlib==3.10.0 seaborn==0.13.2 numpy==1.26.4 tqdm==4.67.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialise Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Customer Behavior Analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Customer Behavior Analysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a7fff5ab10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SGfM4jPrMqer",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+-----------------------+--------+----------------------+--------------------+------------------------+--------------------+-------------+----------------+--------------------+-----------------+-----------------+--------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------------+-------------------------+-----------------------+-------------------+---------------------+--------------+--------------------+--------------------+---------------+------------+------------------+\n",
      "|Survey ResponseID|Order Date|Purchase Price Per Unit|Quantity|Shipping Address State|               Title|ASIN/ISBN (Product Code)|            Category|  Q-demos-age|Q-demos-hispanic|        Q-demos-race|Q-demos-education|   Q-demos-income|Q-demos-gender|Q-sexual-orientation|Q-demos-state|Q-amazon-use-howmany|Q-amazon-use-hh-size|Q-amazon-use-how-oft|Q-substance-use-cigarettes|Q-substance-use-marijuana|Q-substance-use-alcohol|Q-personal-diabetes|Q-personal-wheelchair|Q-life-changes|    Q-sell-YOUR-data|Q-sell-consumer-data|Q-small-biz-use|Q-census-use|Q-research-society|\n",
      "+-----------------+----------+-----------------------+--------+----------------------+--------------------+------------------------+--------------------+-------------+----------------+--------------------+-----------------+-----------------+--------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------------+-------------------------+-----------------------+-------------------+---------------------+--------------+--------------------+--------------------+---------------+------------+------------------+\n",
      "|R_01vNIayewjIIKMF|2018-12-04|                   7.98|     1.0|                    NJ|SanDisk Ultra 16G...|              B0143RTB1E|        FLASH_MEMORY|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          NULL|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "|R_01vNIayewjIIKMF|2018-12-22|                  13.99|     1.0|                    NJ|Betron BS10 Earph...|              B01MA1MJ6H|          HEADPHONES|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          NULL|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "|R_01vNIayewjIIKMF|2018-12-24|                   8.99|     1.0|                    NJ|                NULL|              B078JZTFN3|                NULL|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          NULL|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "|R_01vNIayewjIIKMF|2018-12-25|                  10.45|     1.0|                    NJ|Perfecto Stainles...|              B06XWF9HML|       DISHWARE_BOWL|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          NULL|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "|R_01vNIayewjIIKMF|2018-12-25|                   10.0|     1.0|                    NJ|Proraso Shaving C...|              B00837ZOI0|       SHAVING_AGENT|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          NULL|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "|R_01vNIayewjIIKMF|2019-02-18|                  10.99|     1.0|                    NJ|Micro USB Cable A...|              B01GFB2E9M|  COMPUTER_PROCESSOR|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          NULL|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "|R_01vNIayewjIIKMF|2019-02-18|                   4.99|     1.0|                    NJ|Amazon Basics USB...|              B00NH13S44|     COMPUTER_ADD_ON|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          NULL|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "|R_01vNIayewjIIKMF|2019-04-23|                  12.99|     1.0|                    NJ|Men's Leather Bel...|              B07L84ZZXC|        APPAREL_BELT|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          NULL|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "|R_01vNIayewjIIKMF|2019-04-23|                  24.69|     1.0|                    NJ|                NULL|              B06XKNWJN2|                NULL|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          NULL|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "|R_01vNIayewjIIKMF|2019-05-02|                   9.99|     1.0|                    NJ|UGREEN Tablet Sta...|              B07CG71KQ1|PORTABLE_ELECTRON...|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          NULL|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "|R_01vNIayewjIIKMF|2019-05-02|                  12.79|     1.0|                    NJ|Betron B25 in-Ear...|              B079GFF4HZ|          HEADPHONES|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          NULL|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "|R_01vNIayewjIIKMF|2019-05-11|                   9.14|     1.0|                    NJ|NEW Norpro Instan...|              B01M0Q84BR|    IMMERSION_HEATER|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          NULL|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "|R_01vNIayewjIIKMF|2019-05-16|                   9.99|     1.0|                    NJ|Betron BS10 Earph...|              B01MA1MJ6H|          HEADPHONES|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          NULL|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "|R_01vNIayewjIIKMF|2019-06-15|                  19.99|     1.0|                    NJ|SanDisk 128GB Ult...|              B073JYC4XM|        FLASH_MEMORY|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          NULL|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "|R_01vNIayewjIIKMF|2019-06-15|                  14.72|     1.0|                    NJ|Slippery Stuff, 1...|              B00I7DT454|      BODY_LUBRICANT|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          NULL|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "|R_01vNIayewjIIKMF|2019-06-25|                   9.99|     1.0|                    NJ|Betron BS10 Earph...|              B01MA1MJ6H|          HEADPHONES|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          NULL|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "|R_01vNIayewjIIKMF|2019-07-14|                  26.77|     1.0|                    NJ|Logitech M570 Wir...|              B0043T7FXE|         INPUT_MOUSE|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          NULL|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "|R_01vNIayewjIIKMF|2019-08-02|                  12.99|     1.0|                    NJ|Betron BS10 Earph...|              B01M2V05RE|          HEADPHONES|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          NULL|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "|R_01vNIayewjIIKMF|2019-08-10|                   7.95|     1.0|                    NJ|TUDIA Motorola Mo...|              B07BCZMBNR| CELLULAR_PHONE_CASE|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          NULL|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "|R_01vNIayewjIIKMF|2019-09-06|                  53.95|     1.0|                    NJ|WOWMAX Triangular...|              B075CMYT43|     BODY_POSITIONER|35 - 44 years|             Yes|Black or African ...|Bachelor's degree|$25,000 - $49,999|          Male|heterosexual (str...|   New Jersey|        1 (just me!)|        1 (just me!)|Less than 5 times...|                        No|                       No|                     No|                 No|                   No|          NULL|Yes if I get part...|Yes if consumers ...|             No|          No|               Yes|\n",
      "+-----------------+----------+-----------------------+--------+----------------------+--------------------+------------------------+--------------------+-------------+----------------+--------------------+-----------------+-----------------+--------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------------+-------------------------+-----------------------+-------------------+---------------------+--------------+--------------------+--------------------+---------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the paths to the datasets/csv files\n",
    "amazon_purchases_path = \"amazon-purchases.csv\"\n",
    "survey_path = \"survey.csv\"\n",
    "fields_path = \"fields.csv\"\n",
    "\n",
    "# Load datasets into PySpark DataFrames\n",
    "amazon_purchases = spark.read.csv(amazon_purchases_path, header=True, inferSchema=True)\n",
    "survey = spark.read.csv(survey_path, header=True, inferSchema=True)\n",
    "fields = spark.read.csv(fields_path, header=True, inferSchema=True)\n",
    "\n",
    "# Merge the datasets\n",
    "merged_data = amazon_purchases.join(survey, \"Survey ResponseID\", \"inner\") \n",
    "                              \n",
    "\n",
    "\n",
    "# Display the merged data\n",
    "merged_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Survey ResponseID: string (nullable = true)\n",
      " |-- Order Date: date (nullable = true)\n",
      " |-- Purchase Price Per Unit: double (nullable = true)\n",
      " |-- Quantity: double (nullable = true)\n",
      " |-- Shipping Address State: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- ASIN/ISBN (Product Code): string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Q-demos-age: string (nullable = true)\n",
      " |-- Q-demos-hispanic: string (nullable = true)\n",
      " |-- Q-demos-race: string (nullable = true)\n",
      " |-- Q-demos-education: string (nullable = true)\n",
      " |-- Q-demos-income: string (nullable = true)\n",
      " |-- Q-demos-gender: string (nullable = true)\n",
      " |-- Q-sexual-orientation: string (nullable = true)\n",
      " |-- Q-demos-state: string (nullable = true)\n",
      " |-- Q-amazon-use-howmany: string (nullable = true)\n",
      " |-- Q-amazon-use-hh-size: string (nullable = true)\n",
      " |-- Q-amazon-use-how-oft: string (nullable = true)\n",
      " |-- Q-substance-use-cigarettes: string (nullable = true)\n",
      " |-- Q-substance-use-marijuana: string (nullable = true)\n",
      " |-- Q-substance-use-alcohol: string (nullable = true)\n",
      " |-- Q-personal-diabetes: string (nullable = true)\n",
      " |-- Q-personal-wheelchair: string (nullable = true)\n",
      " |-- Q-life-changes: string (nullable = true)\n",
      " |-- Q-sell-YOUR-data: string (nullable = true)\n",
      " |-- Q-sell-consumer-data: string (nullable = true)\n",
      " |-- Q-small-biz-use: string (nullable = true)\n",
      " |-- Q-census-use: string (nullable = true)\n",
      " |-- Q-research-society: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgFaXMsoO04U"
   },
   "source": [
    "#1. Data Preparation\n",
    "\n",
    "Before analysis, the data needs to be prepared to ensure consistency and efficiency.\n",
    "- Check for data consistency and ensure all columns are correctly formatted.\n",
    "- Structure and prepare the dataset for further processing, ensuring that relevant features are retained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "L2neUeVP3f6t",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+-----------------------+--------+----------------------+-----+------------------------+--------+-----------+----------------+------------+-----------------+--------------+--------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------------+-------------------------+-----------------------+-------------------+---------------------+--------------+----------------+--------------------+---------------+------------+------------------+\n",
      "|Survey ResponseID|Order Date|Purchase Price Per Unit|Quantity|Shipping Address State|Title|ASIN/ISBN (Product Code)|Category|Q-demos-age|Q-demos-hispanic|Q-demos-race|Q-demos-education|Q-demos-income|Q-demos-gender|Q-sexual-orientation|Q-demos-state|Q-amazon-use-howmany|Q-amazon-use-hh-size|Q-amazon-use-how-oft|Q-substance-use-cigarettes|Q-substance-use-marijuana|Q-substance-use-alcohol|Q-personal-diabetes|Q-personal-wheelchair|Q-life-changes|Q-sell-YOUR-data|Q-sell-consumer-data|Q-small-biz-use|Q-census-use|Q-research-society|\n",
      "+-----------------+----------+-----------------------+--------+----------------------+-----+------------------------+--------+-----------+----------------+------------+-----------------+--------------+--------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------------+-------------------------+-----------------------+-------------------+---------------------+--------------+----------------+--------------------+---------------+------------+------------------+\n",
      "|                0|         0|                      0|       0|                 86832|89740|                     951|   89435|          0|               0|           0|                0|             0|             0|                   0|            0|                   0|                   0|                   0|                         0|                        0|                      0|                  0|                    0|       1212958|               0|                   0|              0|           0|                 0|\n",
      "+-----------------+----------+-----------------------+--------+----------------------+-----+------------------------+--------+-----------+----------------+------------+-----------------+--------------+--------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------------+-------------------------+-----------------------+-------------------+---------------------+--------------+----------------+--------------------+---------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as spark_sum, col\n",
    "\n",
    "# Check for missing values in the merged dataset\n",
    "merged_data.select([spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in merged_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of columns.\n",
    "len(merged_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1811238"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of rows in the merged dataset.\n",
    "merged_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-life-changes: 1212958\n",
      "Title: 89740\n",
      "Category: 89435\n",
      "Shipping Address State: 86832\n",
      "ASIN/ISBN (Product Code): 951\n"
     ]
    }
   ],
   "source": [
    "null_counts = merged_data.select([\n",
    "    spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in merged_data.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "non_zero_nulls = {k: v for k, v in null_counts.items() if v > 0}\n",
    "\n",
    "sorted_nulls = dict(sorted(non_zero_nulls.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "for col_name, count in sorted_nulls.items():\n",
    "    print(f\"{col_name}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WS3ZMXS-eNYE"
   },
   "source": [
    "# 2. Data Cleaning.\n",
    "\n",
    "Prepare the data for further analysis by performing data cleaning such as missing value treatment, handle data schema, outlier analysis, and relevant feature engineering techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QanhKxhkC0zp"
   },
   "source": [
    "## 2.1 Handling Missing values.\n",
    "Handle missing values in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zrtZmZ4sR6oX"
   },
   "outputs": [],
   "source": [
    "# Import necessary functions\n",
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "# Fill missing (null) values with the appropriate techniques as required by the analysis\n",
    "\n",
    "# Aggregate and count missing values (nulls) for each column after replacement\n",
    "\n",
    "# Display the count of remaining missing values in each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6Kn4EDZrO31q",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Dropping the column \"Q-life-changes\" as about 67% values are null.\n",
    "merged_data = merged_data.drop(\"Q-life-changes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows where the \"ASIN/ISBN (Product Code)\" column value is null\n",
    "merged_data = merged_data.na.drop(subset=[\"ASIN/ISBN (Product Code)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the missing values in the remaining 3 categorical columns with their respective modes. \n",
    "null_columns = ['Title', 'Category', 'Shipping Address State']\n",
    "for i in null_columns:\n",
    "    mode_row = merged_data.groupBy(i).count().orderBy(desc(\"count\")).first()\n",
    "    \n",
    "    # Check if the mode is None (for instance, if the column has only None values)\n",
    "    if mode_row and mode_row[0] is not None:\n",
    "        mode_value = mode_row[0]\n",
    "        # Fill missing values with the mode\n",
    "        merged_data = merged_data.fillna({i: mode_value})\n",
    "    else:\n",
    "        # If the mode is \"None\".\n",
    "        merged_data = merged_data.fillna({i: \"Unknown\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for any null values after handling them.\n",
    "null_counts = merged_data.select([\n",
    "    spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in merged_data.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "# Filter to show only columns with null values.\n",
    "non_zero_nulls = {k: v for k, v in null_counts.items() if v > 0}\n",
    "\n",
    "# Sort by null count (optional)\n",
    "sorted_nulls = dict(sorted(non_zero_nulls.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Print in a neat format\n",
    "for col_name, count in sorted_nulls.items():\n",
    "    print(f\"{col_name}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are no more columns in the dataframe containing null values, as they are either filled with the mode or \"Unknown\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RIFU8rcSIOU"
   },
   "source": [
    "## 2.2 Feature Engineering.\n",
    "Perform feature engineering on the dataset to extract relevant/ create new features as required and map specific data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "kHhSEkFiSANw",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Survey ResponseID',\n",
       " 'Order Date',\n",
       " 'Purchase Price Per Unit',\n",
       " 'Quantity',\n",
       " 'Shipping Address State',\n",
       " 'Title',\n",
       " 'ASIN/ISBN (Product Code)',\n",
       " 'Category',\n",
       " 'Q-demos-age',\n",
       " 'Q-demos-hispanic',\n",
       " 'Q-demos-race',\n",
       " 'Q-demos-education',\n",
       " 'Q-demos-income',\n",
       " 'Q-demos-gender',\n",
       " 'Q-sexual-orientation',\n",
       " 'Q-demos-state',\n",
       " 'Q-amazon-use-howmany',\n",
       " 'Q-amazon-use-hh-size',\n",
       " 'Q-amazon-use-how-oft',\n",
       " 'Q-substance-use-cigarettes',\n",
       " 'Q-substance-use-marijuana',\n",
       " 'Q-substance-use-alcohol',\n",
       " 'Q-personal-diabetes',\n",
       " 'Q-personal-wheelchair',\n",
       " 'Q-sell-YOUR-data',\n",
       " 'Q-sell-consumer-data',\n",
       " 'Q-small-biz-use',\n",
       " 'Q-census-use',\n",
       " 'Q-research-society',\n",
       " 'year',\n",
       " 'month',\n",
       " 'date']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, month, year, dayofmonth\n",
    "\n",
    "# Perform appropriate feature engineering. Eg. Extract order date, month, year and cast to the appropriate values\n",
    "merged_data = merged_data.withColumn(\"year\", year(\"Order Date\"))\n",
    "merged_data = merged_data.withColumn(\"month\", month(\"Order Date\"))\n",
    "merged_data = merged_data.withColumn(\"date\", dayofmonth(\"Order Date\"))\n",
    "# Display the updated dataset\n",
    "merged_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "WcD1wyX9SKPs",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Survey ResponseID',\n",
       " 'Order Date',\n",
       " 'Purchase Price Per Unit',\n",
       " 'Quantity',\n",
       " 'Shipping Address State',\n",
       " 'Title',\n",
       " 'ASIN/ISBN (Product Code)',\n",
       " 'Category',\n",
       " 'Q-demos-age',\n",
       " 'Q-demos-hispanic',\n",
       " 'Q-demos-race',\n",
       " 'Q-demos-education',\n",
       " 'Q-demos-income',\n",
       " 'Q-demos-gender',\n",
       " 'Q-sexual-orientation',\n",
       " 'Q-demos-state',\n",
       " 'Q-amazon-use-howmany',\n",
       " 'Q-amazon-use-hh-size',\n",
       " 'Q-amazon-use-how-oft',\n",
       " 'Q-substance-use-cigarettes',\n",
       " 'Q-substance-use-marijuana',\n",
       " 'Q-substance-use-alcohol',\n",
       " 'Q-personal-diabetes',\n",
       " 'Q-personal-wheelchair',\n",
       " 'Q-sell-YOUR-data',\n",
       " 'Q-sell-consumer-data',\n",
       " 'Q-small-biz-use',\n",
       " 'Q-census-use',\n",
       " 'Q-research-society',\n",
       " 'year',\n",
       " 'month',\n",
       " 'date',\n",
       " 'income_numeric',\n",
       " 'gender_numeric']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map, lit\n",
    "from itertools import chain\n",
    "\n",
    "# Map categorical income to numerical values\n",
    "income_mapping = {\n",
    "    'Less than $25,000': 0,\n",
    "    '$25,000 - $49,999': 1,\n",
    "    '$50,000 - $74,999': 2,\n",
    "    '$75,000 - $99,999': 3,\n",
    "    '$100,000 - $149,999': 4,\n",
    "    '$150,000 or more': 5\n",
    "}\n",
    "\n",
    "income_map_expr = create_map([lit(x) for x in chain(*income_mapping.items())])\n",
    "\n",
    "# Map gender to numerical values\n",
    "gender_mapping = {\n",
    "    'Male': 1,\n",
    "    'Female': 2,\n",
    "    'Other': 3,\n",
    "    'Prefer not to say': 4\n",
    "}\n",
    "\n",
    "gender_map_expr = create_map([lit(x) for x in chain(*gender_mapping.items())])\n",
    "\n",
    "# Display the updated dataset\n",
    "merged_data = merged_data.withColumn(\"income_numeric\", income_map_expr.getItem(\"income\")) \\\n",
    "              .withColumn(\"gender_numeric\", gender_map_expr.getItem(\"gender\"))\n",
    "\n",
    "merged_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdkrHeIKSTr7"
   },
   "source": [
    "## 2.3 Data Cleaning.\n",
    "Handle data cleaning techniques such as data duplication, dropping unnecessary values etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jq52EEBiSV74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Duplicates: 11506\n",
      "Number of Duplicates After Cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "print(\"Number of Duplicates:\", merged_data.count() - merged_data.dropDuplicates().count())\n",
    "\n",
    "# Remove duplicates\n",
    "merged_data = merged_data.dropDuplicates()\n",
    "\n",
    "# Verify duplicates after cleaning\n",
    "print(\"Number of Duplicates After Cleaning:\", merged_data.count() - merged_data.dropDuplicates().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "c9njYfvDTvQN"
   },
   "outputs": [],
   "source": [
    "# cleaned_data_path =  r\"C:\\Users\\Aayush Singh\"# Save the cleaned dataset locally\n",
    "# merged_data.write.csv(cleaned_data_path, header=True, mode='overwrite')\n",
    "\n",
    "# # Load the cleaned dataset from the location\n",
    "# cleaned_data = spark.read.csv(cleaned_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# # Display the first few rows\n",
    "# print(\"Cleaned Data:\")\n",
    "# cleaned_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zc_-ALrVh5N"
   },
   "source": [
    "# 3. Exploratory Data Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vguqgj3uDOsN"
   },
   "source": [
    "## 3.1 Analyse purchases by hour, day and month.\n",
    "\n",
    "Examine overall trends in purchases over time and analyse the trends by hour, day, month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2N2T4daX_bG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Purchase Distribution by Hour, Day, and Month\n",
    "\n",
    "from pyspark.sql.functions import hour, dayofweek, month\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract hour, day, and month\n",
    "merged_data = merged_data.withColumn('day_of_week', dayofweek('Order Date'))\n",
    "merged_data = merged_data.withColumn('month', month('Order Date'))\n",
    "\n",
    "# Group and count purchases by time factors\n",
    "daily_data = merged_data.groupBy('day_of_week').count().orderBy('day_of_week')\n",
    "monthly_data = merged_data.groupBy('month').count().orderBy('month')\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "daily_df = daily_data.toPandas()\n",
    "monthly_df = monthly_data.toPandas()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Daily distribution plot\n",
    "sns.barplot(x='day_of_week', y='count', data=daily_df, ax=axes[1])\n",
    "axes[1].set_title('Purchases by Day of Week')\n",
    "axes[1].set_xlabel('Day of Week (1=Monday, 7=Sunday)')\n",
    "axes[1].set_ylabel('Number of Purchases')\n",
    "\n",
    "# Monthly distribution plot\n",
    "sns.barplot(x='month', y='count', data=monthly_df, ax=axes[2])\n",
    "axes[2].set_title('Purchases by Month')\n",
    "axes[2].set_xlabel('Month')\n",
    "axes[2].set_ylabel('Number of Purchases')\n",
    "\n",
    "# Plot the data\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j2_MgLtiTv5E"
   },
   "outputs": [],
   "source": [
    "# Monthly Purchase Trends\n",
    "\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "# Extract month and year from 'Order Date'\n",
    "merged_data = merged_data.withColumn('Month_Year', date_format('Order Date', 'yyyy-MM'))\n",
    "\n",
    "# Group by month and count purchases\n",
    "monthly_purchases = merged_data.groupBy('Month_Year').count().orderBy('Month_Year')\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "monthly_purchases_pd = monthly_purchases.toPandas()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(monthly_purchases_pd['Month_Year'], monthly_purchases_pd['count'], marker='o')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Monthly Purchase Trends')\n",
    "plt.xlabel('Month-Year')\n",
    "plt.ylabel('Number of Purchases')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uPGP9f9pyawh"
   },
   "outputs": [],
   "source": [
    "# Yearly Purchase Trends\n",
    "# Group by Year and count purchases\n",
    "yearly_data = merged_data.groupBy('year').count().orderBy('year')\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "yearly_df = yearly_data.toPandas()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(yearly_df['year'], yearly_df['count'], marker='o')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Yearly Purchase Trends')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Purchases')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TckpboEbDTfJ"
   },
   "source": [
    "## 3.2 Customer Demographics vs Purchase Frequency.\n",
    "Analyse the trends between the customer deographics and the purchase frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMfGJhPlX_Yv"
   },
   "outputs": [],
   "source": [
    "# Correlation Between Demographics and Purchase Frequency\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Group by demographic attributes and count purchases\n",
    "demographic_purchases = merged_data.groupBy('Q-demos-gender').agg(F.count('*').alias('purchase_count'))\n",
    "\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "demographic_purchases_pd = demographic_purchases.toPandas()\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(demographic_purchases_pd['Q-demos-gender'], demographic_purchases_pd['purchase_count'], color='skyblue')\n",
    "plt.title('Purchase Frequency by Demographic Group')\n",
    "plt.xlabel('Demographic Group (Gender)')\n",
    "plt.ylabel('Number of Purchases')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXI_SKANDc8b"
   },
   "source": [
    "## 3.3 Purchase behavior weekend vs weekday.\n",
    "\n",
    "Compare the purchase behavior of customer's on weekdays vs. weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iScn1AAZX_V_"
   },
   "outputs": [],
   "source": [
    "# Weekday vs. Weekend Purchase Behavior\n",
    "\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Define weekdays and weekends\n",
    "merged_data = merged_data.withColumn(\"Day_Type\", when(col(\"Order Date\").rlike(\".*[0-6]$\"), \"Weekend\")\n",
    "                   .otherwise(\"Weekday\"))\n",
    "\n",
    "# Group and count purchases\n",
    "purchase_counts = merged_data.groupBy(\"Day_Type\").count()\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "purchase_counts_pd = purchase_counts.toPandas()\n",
    "\n",
    "# Plot\n",
    "purchase_counts_pd.set_index(\"Day_Type\").plot(kind='bar', legend=False, color=['skyblue', 'lightgreen'])\n",
    "plt.title('Weekday vs Weekend Purchase Behavior')\n",
    "plt.ylabel('Number of Purchases')\n",
    "plt.xlabel('Day Type')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ux5eQBRDgSs"
   },
   "source": [
    "## 3.4 Frequently purchased product pairs.\n",
    "\n",
    "Analyze how frequently products are purchased together (also known as Market Basket Analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PuAFZA93X_Tr"
   },
   "outputs": [],
   "source": [
    "# Frequently Purchased Product Pairs (Market Basket Analysis)\n",
    "\n",
    "from pyspark.sql.functions import collect_set, explode\n",
    "\n",
    "# Group purchases by customer and collect items bought together\n",
    "df_grouped = merged_data.groupBy(\"Survey ResponseID\").agg(collect_set(\"Title\").alias(\"Title\"))\n",
    "\n",
    "\n",
    "# Explode item pairs\n",
    "df_exploded = df_grouped.withColumn(\"item\", explode(\"Title\")).select(\"Survey ResponseID\", \"item\")\n",
    "df_exploded.createOrReplaceTempView(\"exploded\")\n",
    "\n",
    "df_pairs = spark.sql(\"\"\"\n",
    "    SELECT a.`Survey ResponseID`, a.item AS item1, b.item AS item2\n",
    "    FROM exploded a\n",
    "    JOIN exploded b\n",
    "    ON a.`Survey ResponseID` = b.`Survey ResponseID`\n",
    "    AND a.item < b.item\n",
    "\"\"\")\n",
    "\n",
    "# Count co-occurrences of items\n",
    "df_pair_counts = df_pairs.groupBy(\"item1\", \"item2\").count().sort(\"count\", ascending=False)\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "pandas_df = df_pair_counts.toPandas()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_pairs['item1'] + \" & \" + top_pairs['item2'], top_pairs['count'], color='skyblue')\n",
    "plt.xlabel('Count')\n",
    "plt.title('Top 10 Most Frequently Bought Item Pairs')\n",
    "plt.gca().invert_yaxis()  # To have the highest values at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VeD_yjU8DlEN"
   },
   "source": [
    "## 3.5 Examine Product Performance.\n",
    "\n",
    "Examine the performance of products by calculating revenue and item popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vr0fRcnSTvhE"
   },
   "outputs": [],
   "source": [
    "# Contribution of Product Categories (Top 25)\n",
    "df_with_sales = merged_data.withColumn(\"total_sales\", col(\"Purchase Price Per Unit\") * col(\"Quantity\"))\n",
    "category_contribution = df_with_sales.groupBy(\"Category\").sum(\"total_sales\").withColumnRenamed(\"sum(total_sales)\", \"total_sales\")\n",
    "category_contribution = category_contribution.orderBy(\"total_sales\", ascending=False)\n",
    "\n",
    "# Show the top 25 categories\n",
    "top_25_contributions = category_contribution.limit(25)\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "top_25_contributions_pd = top_25_contributions.toPandas()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"total_sales\", y=\"Category\", data=top_25_contributions_pd, palette=\"viridis\")\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(\"Total Sales\")\n",
    "plt.ylabel(\"Product Category\")\n",
    "plt.title(\"Top 25 Product Categories by Total Sales\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9t7-ax4Dr7w"
   },
   "source": [
    "## 3.6 Top products by quantity.\n",
    "\n",
    "Identify the most frequently purchased products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnkrF7d8XAxt"
   },
   "outputs": [],
   "source": [
    "# Top 10 Products by Quantity\n",
    "\n",
    "# Group by product title and sum 'Quantity'\n",
    "product_quantity = merged_data.groupBy(\"Title\").sum(\"Quantity\").withColumnRenamed(\"sum(quantity)\", \"total_quantity\")\n",
    "product_quantity = product_quantity.orderBy(\"total_quantity\", ascending=False)\n",
    "top_10_products = product_quantity.limit(10)\n",
    "\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "top_10_products_pd = top_10_products.toPandas()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"total_quantity\", y=\"Title\", data=top_10_products_pd, palette=\"Blues_d\")\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(\"Total Quantity Sold\")\n",
    "plt.ylabel(\"Product Title\")\n",
    "plt.title(\"Top 10 Products by Quantity Sold\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucWGxDLtDuH9"
   },
   "source": [
    "## 3.7 Distribution of Purchases by State.\n",
    "\n",
    "Analyze the distribution of purchases across states and categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03-YzrUMXbpC"
   },
   "outputs": [],
   "source": [
    "# Distribution of Purchases by State (Top 25)\n",
    "state_purchases = merged_data.groupBy(\"Shipping Address State\").sum(\"Quantity\").withColumnRenamed(\"sum(quantity)\", \"total_purchases\")\n",
    "state_purchases = state_purchases.orderBy(\"total_purchases\", ascending=False)\n",
    "top_25_states = state_purchases.limit(25)\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "top_25_states_pd = top_25_states.toPandas()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"total_purchases\", y=\"Shipping Address State\", data=top_25_states_pd, palette=\"viridis\")\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(\"Total Purchases\")\n",
    "plt.ylabel(\"State\")\n",
    "plt.title(\"Top 25 States by Total Purchases\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKHw1EmGDwyo"
   },
   "source": [
    "## 3.8 Price vs Product Quantity.\n",
    "\n",
    "Identify the Relationship between Price and Quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0_KzBl2X6p6"
   },
   "outputs": [],
   "source": [
    "# Relationship Between Price and Quantity\n",
    "df_price_quantity = merged_data.select(\"Purchase Price Per Unit\", \"Quantity\")\n",
    "\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "df_price_quantity_pd = df_price_quantity.toPandas()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=\"Purchase Price Per Unit\", y=\"Quantity\", data=df_price_quantity_pd, color='blue', alpha=0.6)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Quantity Sold\")\n",
    "plt.title(\"Relationship Between Price and Quantity\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a37zLfoyD39R"
   },
   "source": [
    "## 3.9 Analyse the spending KPIs.\n",
    "\n",
    "\n",
    "A popular KPI is average spend per customer. Calculate this metric as the ratio of total transaction amount from non-recurring payments divided by the total number of customers who made a purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U44bgKSgX6h6"
   },
   "outputs": [],
   "source": [
    "# Average Spend per Customer\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Group by customer and calculate average spend\n",
    "df_with_spend = merged_data.withColumn(\"total_spend\", col(\"Purchase Price Per Unit\") * col(\"Quantity\"))\n",
    "customer_spend = df_with_spend.groupBy(\"Survey ResponseID\").sum(\"total_spend\").withColumnRenamed(\"sum(total_spend)\", \"total_spend\")\n",
    "average_spend_per_customer = customer_spend.agg(avg(\"total_spend\").alias(\"avg_spend\"))\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "average_spend_df = average_spend_per_customer.toPandas()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(customer_spend.toPandas()[\"total_spend\"], kde=True, color=\"blue\")\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(\"Total Spend per Customer ($)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Spend per Customer\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPMmGFtFPzdn"
   },
   "source": [
    "Analyse the Repeat Purchase Behavior of Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evh5cQ2gX6ai"
   },
   "outputs": [],
   "source": [
    "# Repeat Purchase Analysis Behavior Per Customers\n",
    "customer_purchase_count = merged_data.groupBy(\"Survey ResponseID\").count().withColumnRenamed(\"count\", \"purchase_count\")\n",
    "repeat_customers = customer_purchase_count.filter(col(\"purchase_count\") > 1)\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "repeat_customers_pd = repeat_customers.toPandas()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(repeat_customers_pd[\"purchase_count\"], kde=False, bins=range(1, repeat_customers_pd[\"purchase_count\"].max() + 1), color=\"blue\")\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(\"Number of Purchases\")\n",
    "plt.ylabel(\"Number of Customers\")\n",
    "plt.title(\"Distribution of Repeat Purchases per Customer\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcoJFv8TP7ZJ"
   },
   "source": [
    "Analyse the top 10 high-engagement customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tRHnpItLX6KG"
   },
   "outputs": [],
   "source": [
    "# Top 10 High-Engagement Customers\n",
    "df_with_spend = merged_data.withColumn(\"total_spend\", col(\"Purchase Price Per Unit\") * col(\"Quantity\"))\n",
    "customer_spend = df_with_spend.groupBy(\"Survey ResponseID\").sum(\"total_spend\").withColumnRenamed(\"sum(total_spend)\", \"total_spend\")\n",
    "top_10_customers = customer_spend.orderBy(\"total_spend\", ascending=False).limit(10)\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "top_10_customers_pd = top_10_customers.toPandas()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"total_spend\", y=\"Survey ResponseID\", data=top_10_customers_pd, palette=\"viridis\")\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(\"Total Spend ($)\")\n",
    "plt.ylabel(\"Customer ID\")\n",
    "plt.title(\"Top 10 High-Engagement Customers by Total Spend\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MccaRxN8EAt6"
   },
   "source": [
    "## 3.10 Seasonal trends in product purchases and their impact on revenues.\n",
    "\n",
    "Investigate the seasonal trends in product purchases and their impact on the overall revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVQf40UghaOZ"
   },
   "outputs": [],
   "source": [
    "# Seasonal Trends in Product Purchases and Their Impact on Revenue\n",
    "\n",
    "from pyspark.sql.functions import year\n",
    "\n",
    "# Extract year and month\n",
    "\n",
    "\n",
    "# Group by year and month, summing total revenue\n",
    "df_with_revenue = merged_data.withColumn(\"total_revenue\", col(\"Purchase Price Per Unit\") * col(\"Quantity\"))\n",
    "\n",
    "# Group by 'year' and 'month' and calculate total revenue per month\n",
    "monthly_revenue = df_with_revenue.groupBy(\"year\", \"month\").sum(\"total_revenue\").withColumnRenamed(\"sum(total_revenue)\", \"total_revenue\")\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "monthly_revenue_pd = monthly_revenue.toPandas()\n",
    "\n",
    "# Plot\n",
    "monthly_revenue_pd[\"year_month\"] = monthly_revenue_pd[\"year\"].astype(str) + \"-\" + monthly_revenue_pd[\"month\"].astype(str).str.zfill(2)\n",
    "\n",
    "# Plotting the seasonal trends in total revenue\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=\"year_month\", y=\"total_revenue\", data=monthly_revenue_pd, marker='o', color=\"blue\")\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(\"Year-Month\")\n",
    "plt.ylabel(\"Total Revenue ($)\")\n",
    "plt.title(\"Seasonal Trends in Product Purchases and Their Impact on Revenue\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAw6x6V0EFSZ"
   },
   "source": [
    "## 3.11 Customer location vs purchasing behavior.\n",
    "\n",
    "Examine the relationship between customer's location and their purchasing behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCH3Uzt7hjBw"
   },
   "outputs": [],
   "source": [
    "# Relationship Between Customer Location and Purchase Behavior\n",
    "df_with_revenue = merged_data.withColumn(\"total_revenue\", col(\"Purchase Price Per Unit\") * col(\"Quantity\"))\n",
    "\n",
    "# Group purchases by state and total spend\n",
    "state_revenue = df_with_revenue.groupBy(\"Shipping Address State\").sum(\"total_revenue\").withColumnRenamed(\"sum(total_revenue)\", \"total_revenue\")\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "state_revenue_pd = state_revenue.toPandas()\n",
    "\n",
    "# Plot revenue by state\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"total_revenue\", y=\"Shipping Address State\", data=state_revenue_pd, palette=\"viridis\")\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(\"Total Revenue ($)\")\n",
    "plt.ylabel(\"State\")\n",
    "plt.title(\"Total Revenue by State\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAI9P9w3bHzd"
   },
   "source": [
    "## 3. Customer Segmentation and Insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmwsPLAScx70"
   },
   "source": [
    "## 3.1 Perform RFM Analysis.\n",
    "\n",
    "RFM Analysis is a powerful customer segmentation technique used to evaluate and quantify customer value based on three key dimensions:\n",
    "- **Recency**,\n",
    "- **Frequency**,\n",
    "- **Monetary**.\n",
    "\n",
    "This method is particularly effective in identifying high-value customers, optimizing marketing strategies, and improving customer retention in the e-commerce industry.\n",
    "\n",
    "\n",
    "### 1. Recency (R)\n",
    "Recency measures how recently a customer made a purchase Customers who have purchased more recently are more likely to respond to promotions and offers.\n",
    "- **Application:** By ranking customers based on the number of days since their last transaction, you can prioritize those who are most engaged.\n",
    "\n",
    "### 2. Frequency (F)\n",
    "Frequency counts the number of purchases a customer has made over a given period.\n",
    "Frequent purchasers tend to be more loyal and are often a source of recurring revenue.\n",
    "- **Application:** Analyzing purchase frequency helps in identifying consistent buyers and understanding their buying patterns.\n",
    "\n",
    "### 3. Monetary (M)\n",
    "Monetary value represents the total amount of money a customer has spent.\n",
    "Customers who spend more are often more profitable, making them ideal targets for retention and upsell strategies.\n",
    "- **Application:** By assessing the monetary contribution, you can distinguish between high-value and low-value customers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ql_svp5od2e6"
   },
   "source": [
    "### Prepare data for RFM Analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ArqthBSaAG7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, max, count, sum, lit\n",
    "\n",
    "# Get the latest order date in the dataset\n",
    "latest_order_date = merged_data.agg(max(\"Order Date\")).collect()[0][0]\n",
    "print(f\"Latest Order Date: {latest_order_date}\")\n",
    "\n",
    "# Calculate RFM metrics\n",
    "rfm_data = merged_data.groupBy(\"Survey ResponseID\") \\\n",
    "    .agg(\n",
    "        # Recency: Difference between the latest date and the last purchase date\n",
    "        datediff(lit(latest_order_date), max(\"Order Date\")).alias(\"Recency\"),\n",
    "        \n",
    "        # Frequency: Count of orders per customer\n",
    "        count(\"*\").alias(\"Frequency\"),\n",
    "        \n",
    "        # Monetary: Sum of total spend (price * quantity)\n",
    "        sum(col(\"Purchase Price Per Unit\") * col(\"Quantity\")).alias(\"Monetary\")\n",
    "    )\n",
    "\n",
    "# Filter out customers with no purchases\n",
    "rfm_data_filtered = rfm_data.filter(col(\"Frequency\") > 0)\n",
    "\n",
    "# Show RFM data\n",
    "rfm_data.show()\n",
    "rfm_data_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIVvr5LUcz1m"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import log1p\n",
    "\n",
    "# Apply log transformation to skewed features\n",
    "skewed_columns = ['Purchase Price Per Unit', 'Quantity']\n",
    "\n",
    "df_log_transformed = merged_data\n",
    "for column in skewed_columns:\n",
    "    df_log_transformed = df_log_transformed.withColumn(column, log1p(col(column)))\n",
    "    \n",
    "# Convert to Pandas DataFrame (for scikit-learn compatibility)\n",
    "df_log_transformed_pd = df_log_transformed.toPandas()\n",
    "\n",
    "# Scale features using StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "features = df_log_transformed_pd[['Purchase Price Per Unit', 'Quantity']]\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the features\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Convert the scaled features back to a DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-pcKc1fdilX"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "X = scaled_df.values\n",
    "# Calculate the Within-Cluster Sum of Squares (WCSS)\n",
    "wcss = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "# Plot the elbow curve with the number of clusters on the x-axis and WCSS on the y-axis\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(1, 11), wcss, marker='o')\n",
    "plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS (Within-Cluster Sum of Squares)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSdWz7-tEbEn"
   },
   "source": [
    "### 3.2.1 When to schedule effective promotions.\n",
    "\n",
    "Compare sales across weekdays to schedule effective promotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJBvN0sqB1Zg"
   },
   "outputs": [],
   "source": [
    "#Compare sales across weekdays to schedule effective promotions\n",
    "\n",
    "from pyspark.sql.functions import dayofweek\n",
    "df_with_sales = merged_data.withColumn('total_sales', F.col('Purchase Price Per Unit') * F.col('Quantity'))\n",
    "\n",
    "# Extract day of the week (1 = Sunday, 7 = Saturday)\n",
    "df_with_weekday = df_with_sales.withColumn('weekday', F.dayofweek('Order Date'))\n",
    "\n",
    "# Group by weekday and sum total sales\n",
    "df_sales_by_weekday = df_with_weekday.groupBy('weekday').agg(F.sum('total_sales').alias('total_sales'))\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "df_sales_by_weekday_pd = df_sales_by_weekday.toPandas()\n",
    "\n",
    "weekday_map = {\n",
    "    1: 'Sunday',\n",
    "    2: 'Monday',\n",
    "    3: 'Tuesday',\n",
    "    4: 'Wednesday',\n",
    "    5: 'Thursday',\n",
    "    6: 'Friday',\n",
    "    7: 'Saturday'\n",
    "}\n",
    "\n",
    "df_sales_by_weekday_pd['weekday'] = df_sales_by_weekday_pd['weekday'].map(weekday_map)\n",
    "\n",
    "# Sort by weekday for visualization\n",
    "df_sales_by_weekday_pd = df_sales_by_weekday_pd.sort_values('weekday', key=lambda x: x.map(weekday_map))\n",
    "\n",
    "# Plot sales by weekday\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(df_sales_by_weekday_pd['weekday'], df_sales_by_weekday_pd['total_sales'])\n",
    "plt.title('Sales by Weekday')\n",
    "plt.xlabel('Weekday')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_efTQMMEeGy"
   },
   "source": [
    "### 3.2.2 Top-selling Products.\n",
    "\n",
    "Identify top-selling products by considering revenue and engagement metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YV0otvIEnJfa"
   },
   "outputs": [],
   "source": [
    "#Identify top-selling products using revenue and engagement metrics\n",
    "df_with_revenue = merged_data.withColumn('total_revenue', F.col('Purchase Price Per Unit') * F.col('Quantity'))\n",
    "\n",
    "# Group by product and sum revenue\n",
    "df_revenue_by_product = df_with_revenue.groupBy('Title').agg(F.sum('total_revenue').alias('total_revenue'))\n",
    "\n",
    "# Get top 10 products by revenue\n",
    "df_top_10_products = df_revenue_by_product.orderBy(F.col('total_revenue').desc()).limit(10)\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "df_top_10_products_pd = df_top_10_products.toPandas()\n",
    "\n",
    "# Plot top products by revenue\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(df_top_10_products_pd['Title'], df_top_10_products_pd['total_revenue'])\n",
    "plt.title('Top 10 Products by Revenue')\n",
    "plt.xlabel('Product Name')\n",
    "plt.ylabel('Total Revenue')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqygUc8JEf7v"
   },
   "source": [
    "### 3.2.3 State-wise revenue Distribution.\n",
    "\n",
    "Assess state-wise revenue to focus on high-growth areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yefqKQzOqiYI"
   },
   "outputs": [],
   "source": [
    "#Assess state-wise revenue to focus on high-growth areas\n",
    "df_with_revenue = merged_data.withColumn('total_revenue', F.col('Purchase Price Per Unit') * F.col('Quantity'))\n",
    "\n",
    "# Group by state and sum revenue\n",
    "df_revenue_by_state = df_with_revenue.groupBy('Shipping Address State').agg(F.sum('total_revenue').alias('total_revenue'))\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "df_revenue_by_state_pd = df_revenue_by_state.toPandas()\n",
    "\n",
    "# Plot revenue by state\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(df_revenue_by_state_pd['Shipping Address State'], df_revenue_by_state_pd['total_revenue'])\n",
    "plt.title('Revenue by State')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Total Revenue')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1ANO3B7EhYQ"
   },
   "source": [
    "### 3.2.4 Repeat Purchase Behavior.\n",
    "\n",
    "Examine repeat purchase behavior to enhance retention initiatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1yhhY0Fr5TT"
   },
   "outputs": [],
   "source": [
    "#Examine repeat purchase behavior to enhance retention initiatives\n",
    "\n",
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "# Count total purchases per customer\n",
    "df_purchases_per_customer = merged_data.groupBy('Survey ResponseID').agg(F.count('*').alias('purchase_count'))\n",
    "\n",
    "# Filter for repeat customers (those with more than one purchase)\n",
    "df_repeat_customers = df_purchases_per_customer.filter(F.col('purchase_count') > 1)\n",
    "\n",
    "# Show sample data\n",
    "df_repeat_customers.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjMwmoHaEjCI"
   },
   "source": [
    "### 3.2.5 Flagging Potential Fraud.\n",
    "\n",
    "Identify irregular transaction patterns to flag potential fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mvm_gMIsmDe"
   },
   "outputs": [],
   "source": [
    "#Identify irregular transaction patterns to flag potential fraud\n",
    "\n",
    "from pyspark.sql.functions import col, avg, stddev\n",
    "\n",
    "# Calculate the threshold for unusually high spending\n",
    "df_with_spending = merged_data.withColumn('total_spent', F.col('Purchase Price Per Unit') * F.col('Quantity'))\n",
    "\n",
    "mean_spent, stddev_spent = df_with_spending.select(\n",
    "    F.avg('total_spent').alias('mean_spent'),\n",
    "    F.stddev('total_spent').alias('stddev_spent')\n",
    ").first()\n",
    "\n",
    "threshold = mean_spent + 3 * stddev_spent\n",
    "\n",
    "df_suspicious_transactions = df_with_spending.filter(F.col('total_spent') > threshold)\n",
    "\n",
    "df_suspicious_transactions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELXXdmsPEkZ3"
   },
   "source": [
    "### 3.2.6 Demand Variations across product categories.\n",
    "\n",
    "Perform inventory management by monitoring demand variations across product categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6XbTQ-sIuFt1"
   },
   "outputs": [],
   "source": [
    "#Monitor demand variations across product categories (Top 25) for inventory management\n",
    "\n",
    "from pyspark.sql.functions import col, sum\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df_with_revenue = merged_data.withColumn('total_revenue', F.col('Purchase Price Per Unit') * F.col('Quantity'))\n",
    "\n",
    "df_with_revenue = df_with_revenue.withColumn('year_month', F.date_format('Order Date', 'yyyy-MM'))\n",
    "\n",
    "# Group by category and month, summing total revenue\n",
    "category_trends = df_with_revenue.groupBy('Category', 'year_month').agg(F.sum('total_revenue').alias('total_revenue'))\n",
    "\n",
    "# Compute total revenue per category\n",
    "category_total_revenue = category_trends.groupBy('Category').agg(F.sum('total_revenue').alias('total_revenue'))\n",
    "\n",
    "# Get the top 25 categories by total revenue\n",
    "top_25_categories = category_total_revenue.orderBy(F.col('total_revenue').desc()).limit(25)\n",
    "\n",
    "# Filter category_trends to include only top 25 categories\n",
    "top_25_category_trends = category_trends.join(top_25_categories, on='Category', how='inner')\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "top_25_category_trends_pd = top_25_category_trends.toPandas()\n",
    "\n",
    "# Plot revenue trends for top 25 categories\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.lineplot(data=top_25_category_trends_pd, x='year_month', y='total_revenue', hue='Category', marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_JGx9raEl1v"
   },
   "source": [
    "### 3.2.7 Assess how bulk purchases affect revenue and supply chain operations.\n",
    "\n",
    "Analyse the impact of how bulk purchasing behavior affects revenue and the overall supply chain operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06SrupNpurIR"
   },
   "outputs": [],
   "source": [
    "#Assess how bulk purchases affect revenue and supply chain operations\n",
    "df_with_revenue = merged_data.withColumn('total_revenue', F.col('Purchase Price Per Unit') * F.col('Quantity'))\n",
    "\n",
    "# Filter bulk purchases (Quantity > 5) and compute total revenue per category\n",
    "df_bulk_purchases = df_with_revenue.filter(F.col('Quantity') > 5)\n",
    "bulk_purchases_by_category = df_bulk_purchases.groupBy('Category').agg(F.sum('total_revenue').alias('total_revenue'))\n",
    "\n",
    "# Select the top 25 categories by total revenue\n",
    "top_25_bulk_categories = bulk_purchases_by_category.orderBy(F.col('total_revenue').desc()).limit(25)\n",
    "\n",
    "# Convert to Pandas for visualisation\n",
    "top_25_bulk_categories_pd = top_25_bulk_categories.toPandas()\n",
    "\n",
    "# Plot revenue from bulk purchases (Top 25 categories)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=top_25_bulk_categories_pd, x='Category', y='total_revenue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JYvEA5GEpLw"
   },
   "source": [
    "### 4. Conclusion\n",
    "\n",
    "Write your conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nYhs8T4w4jI"
   },
   "outputs": [],
   "source": [
    "\tThe number of purchases made by customers has been mostly at consistent levels throughout the week with the peak at Tuesday.\n",
    "\tWhen it comes to purchases throughout the months the most purchases were made in the month of December and the lowest in February.\n",
    "\tThe highest volume of orders was made from the state of California followed by Texas and Florida. On the other hand, Nevada, Kentucky recorded the least purchase amount.\n",
    "\tIf we observe the top sold product categories from amazon then ABIS_BOOK, GIFT_CARD and PET_FOOD were leading the list with TABLET_COMPUTER, ELECTRONIC_CABLE and MATTRESS in the end of the list.\n",
    "\tExtending the third point, the Total Revenue by States was naturally highest for California, Texas and Florida. \n",
    "\tThe number of products ordered is inversely proportional to number of customer using amazon service.\n",
    "\tUpon visualizing the Purchase Frequency based on the gender demographics of the customers, we found that females make more frequent purchases on amazon compared to males and other genders.\n",
    "\tAnother significant observation was that quantity of orders by customers was way more during weekends as opposed to weekdays, which is understandable as people have more time to make various purchases in the weekends compared to working weekdays."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1sZY6vIiWEPOWoKajaiSQ5yGTHXUWhKxg",
     "timestamp": 1739959540968
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
